{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment Review\n",
    "\n",
    "First up:\n",
    "\n",
    "## Natural Language Processing Practice\n",
    "\n",
    "Using the 'Spooky Authors' dataset: https://www.kaggle.com/c/spooky-author-identification/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student 1:**\n",
    "\n",
    "Please grab the dataset and look at a few aspects of this dataset (shape, some examples, etc). We'll be using just the train csv for this, for ease of use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the train set from the competition \n",
    "df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding our target from author initials to numbers\n",
    "le = LabelEncoder()\n",
    "df['target'] = le.fit_transform(df['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking that change\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing our inputs and target\n",
    "X = df['text']\n",
    "y = df['target']\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing a list of stopwords from NLTK, imported above\n",
    "# We're also using the string library add punctuation to our list\n",
    "stopwords_list = stopwords.words('english') + list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student 2:**\n",
    "\n",
    "What is the point of a list of stopwords? How/why will we use this list?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Bag of Words\" - Count Vectorizer\n",
    "\n",
    "Useful link to the 'User Guide' part of the documentation on this: https://scikit-learn.org/stable/modules/feature_extraction.html#common-vectorizer-usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intstantiating our vectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Training on the train set, then transforming the train set\n",
    "X_count_train = count_vectorizer.fit_transform(X_train)\n",
    "# Transforming the test set\n",
    "X_count_test = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating a classifier to use on this text - Multinomial Naive Bayes\n",
    "nb_classifier = MultinomialNB() \n",
    "\n",
    "# Fitting the classifier\n",
    "nb_classifier.fit(X_count_train, y_train)\n",
    "\n",
    "# Getting our predictions for the train and test sets\n",
    "train_preds = nb_classifier.predict(X_count_train)\n",
    "test_preds = nb_classifier.predict(X_count_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's see how we did!\n",
    "print(accuracy_score(y_test, test_preds))\n",
    "plot_confusion_matrix(nb_classifier, X_count_test, y_test, \n",
    "                      values_format = \".4g\") # to make numbers readable\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student 3:**\n",
    "\n",
    "Discuss! How did we do? What could we change?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're about to try this on a few different vectorizers, so let's make that easier!\n",
    "\n",
    "**Student 4:**\n",
    "\n",
    "Write a function where we can provide an instantiated vectorizer, an instantiated classifer, and all of our train and test data, and the function will spit out the accuracy score and confusion matrix just like above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_vectorized_text(vectorizer, classifier, Xtrain, Xtest, ytrain, ytest):\n",
    "    '''\n",
    "    Fit and transform text data using the provided vectorizer, then fit and \n",
    "    predict with the provided classifier, in order to see the resulting\n",
    "    accuracy score and confusion matrix\n",
    "    For the Xtrain, Xtest, ytrain, ytest, expect the output of an\n",
    "    sklearn train/test split\n",
    "    -\n",
    "    Inputs:\n",
    "    vectorizer: an instantiated sklearn vectorizer\n",
    "    classifier: an instantiated sklearn classifier\n",
    "    X_train: training input data\n",
    "    X_test: testing input data\n",
    "    y_train: training true result\n",
    "    y_test: testing true result\n",
    "    -\n",
    "    Outputs: \n",
    "    train_preds: predicted results for the train set\n",
    "    test_preds: predicted results for the test set\n",
    "    '''\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student 5:**\n",
    "\n",
    "Please add in something that was missing from our first Count Vectorizer:\n",
    "\n",
    "Link to the documentation: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new vectorizer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student 6:**\n",
    "\n",
    "Please create a new classifier and compare the results, using our previously-defined function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new classifier here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare: \n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF: Term-Frequency - Inverse Document-Frequency\n",
    "\n",
    "Bryan talked about this... but what even is it?\n",
    "\n",
    "From [the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html):\n",
    "\n",
    "> \"The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.\"\n",
    "\n",
    "Basically, it's a statistic that hopefully reflects how important a word is in the document. By looking at the overall frequency you find how common a word is across the whole corpus, compared to the document frequency that shows how common a word is within the document in question. If a word appears often in our document, but relatively rarely in the corpus, it probably captures an important word in that specific document!\n",
    "\n",
    "In this example, the training corpus is every sentence in the `text` column in our train set, and the document is the individual sentence that we're trying to classify (per row).\n",
    "\n",
    "Reference: http://www.tfidf.com/\n",
    "\n",
    "We'll be using Sklearn's [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html), which is 'equivalent to CountVectorizer followed by TfidfTransformer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the vectorizer\n",
    "tfidf = TfidfVectorizer(stop_words=stopwords_list, use_idf=True)\n",
    "\n",
    "# Training on the train set, then transforming the train set\n",
    "tfidf_train = tfidf.fit_transform(X_train)\n",
    "# Transforming the test set\n",
    "tfidf_test = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(tfidf.idf_, index=tfidf.get_feature_names(),columns=[\"idf_weights\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df.sort_values(by='idf_weights', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's look at a specific example for one row\n",
    "tfidf_test_df = pd.DataFrame(tfidf_test.toarray(), columns=tfidf.vocabulary_.keys())\n",
    "\n",
    "test_doc = tfidf_test_df.iloc[16]\n",
    "print(test_doc.idxmax(axis=1))\n",
    "print(test_doc[test_doc.idxmax(axis=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells you that for the 17th document in our test set, the word 'chivalry' has the highest TF-IDF value.\n",
    "\n",
    "**Student 7:**\n",
    "\n",
    "What does this tell you about the word \"chivalry\" in the this document of our test set?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using our function to compare the results...\n",
    "tfidf = TfidfVectorizer(stop_words=stopwords_list, use_idf=True)\n",
    "nb_tfidf = MultinomialNB()\n",
    "\n",
    "tfidf_train_preds, tfidf_test_preds = classify_vectorized_text(tfidf, nb_tfidf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare:\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use our function to try different classifiers\n",
    "tfidf = TfidfVectorizer(stop_words=stopwords_list, use_idf=True)\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "rfc_train_preds, rfc_test_preds = classify_vectorized_text(tfidf, rfc, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare: \n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Review!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](kmeans.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student 8:**\n",
    "\n",
    "Please describe the steps of a k-means clustering algorithm:\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pca.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student 9:**\n",
    "\n",
    "Please describe how principal component analysis works:\n",
    "\n",
    "- \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
